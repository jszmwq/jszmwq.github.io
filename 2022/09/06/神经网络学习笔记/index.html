<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>神经网络学习笔记 | j52nnw9的博客</title><meta name="keywords" content="计科课程"><meta name="author" content="萌新QAQ"><meta name="copyright" content="萌新QAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="神经网络 第一章 人工神经网络绪论1.1人工智能的含义对人工智能的研究必然借鉴自然智能—人脑的研究成果，根据侧重点的不同，可分为三大类：   结构模拟：神经计算，生理学派，连接主义  生理学派  根据人脑的生理结构和工作机理，实现计算机的智能，是一种局部和近似的模拟(ANN)  特点：  利用NN的自学习能力获取知识，再利用知识解决问题 具有高度的并行性、分布性、很强的鲁棒性和容错性 擅长模拟人脑">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络学习笔记">
<meta property="og:url" content="https://jszmwq.github.io/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="j52nnw9的博客">
<meta property="og:description" content="神经网络 第一章 人工神经网络绪论1.1人工智能的含义对人工智能的研究必然借鉴自然智能—人脑的研究成果，根据侧重点的不同，可分为三大类：   结构模拟：神经计算，生理学派，连接主义  生理学派  根据人脑的生理结构和工作机理，实现计算机的智能，是一种局部和近似的模拟(ANN)  特点：  利用NN的自学习能力获取知识，再利用知识解决问题 具有高度的并行性、分布性、很强的鲁棒性和容错性 擅长模拟人脑">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jszmwq.github.io/img/Animation/5.png">
<meta property="article:published_time" content="2022-09-06T08:52:28.000Z">
<meta property="article:modified_time" content="2023-02-13T06:48:57.423Z">
<meta property="article:author" content="萌新QAQ">
<meta property="article:tag" content="计科课程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jszmwq.github.io/img/Animation/5.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://jszmwq.github.io/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '神经网络学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-13 14:48:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a></div></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/Animation/5.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">j52nnw9的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">神经网络学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-06T08:52:28.000Z" title="发表于 2022-09-06 16:52:28">2022-09-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-13T06:48:57.423Z" title="更新于 2023-02-13 14:48:57">2023-02-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="神经网络学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="神经网络-第一章-人工神经网络绪论"><a href="#神经网络-第一章-人工神经网络绪论" class="headerlink" title="神经网络 第一章 人工神经网络绪论"></a>神经网络 第一章 人工神经网络绪论</h2><h3 id="1-1人工智能的含义"><a href="#1-1人工智能的含义" class="headerlink" title="1.1人工智能的含义"></a>1.1人工智能的含义</h3><p>对人工智能的研究必然借鉴自然智能—人脑的研究成果，根据侧重点的不同，可分为三大类： </p>
<ul>
<li><p>结构模拟：神经计算，生理学派，连接主义</p>
<ul>
<li><p>生理学派</p>
<ul>
<li>根据人脑的生理结构和工作机理，实现计算机的智能，是一种局部和近似的模拟(ANN)</li>
</ul>
<p>特点：</p>
<ul>
<li>利用NN的自学习能力获取知识，再利用知识解决问题</li>
<li>具有高度的并行性、分布性、很强的鲁棒性和容错性</li>
<li>擅长模拟人脑的形象思维，便于实现人脑的低级感知功能：图象、语音的识别和处理</li>
</ul>
</li>
</ul>
</li>
<li><p>功能模拟：符号推演，心理学派，符号主义</p>
<ul>
<li>心理学派<ul>
<li>根据人脑的心理模型，将知识/问题表示成某种逻辑网络，采用符号推演的方法，实现搜索、 推理、学习等功能。如自动机器推理、定理证明、专家系统、机器博弈等</li>
<li>擅长模拟人脑的逻辑思维，便于实现人脑的高 级认知功能（推理、决策等）</li>
</ul>
</li>
</ul>
</li>
<li><p>行为模拟：控制进化，控制论学派，行为主义，进化主义</p>
<ul>
<li>控制论学派<ul>
<li>基于“感知—行为模型” ，模拟人在控制过程中的智能活动和行为特征：自优化、自适应、自学习、自组织等</li>
<li>也可称为现场AI（Situated AI），强调智能系统与环境的交互</li>
<li>认为智能取决于感知和行动，智能行为不需要知识，认为人的智能、机器的智能可以逐步进化， 但必须与现实交互</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="1-3人工神经网络的概念"><a href="#1-3人工神经网络的概念" class="headerlink" title="1.3人工神经网络的概念"></a>1.3人工神经网络的概念</h3><p>Artificial Neural Networks，简记作ANN</p>
<blockquote>
<p>传统的AI技术模拟左脑的逻辑思维</p>
<p>ANN技术模拟右脑的形象思维</p>
</blockquote>
<p>人工神经网络是受生物大脑启发，基于模拟生物大脑的结构和功能，采用数学和物理方法进行研究而构成的一种信息处理系统。它是由许多非常简单的并行工作的处理单元按照某种方法相互连接，并依靠其状态对外部输入信息进 行动态响应的计算机系统。</p>
<p>大脑是由大量神经细胞或神经元组成的，每个神经元可以被看作是一个小的处理单元，这些神经元按照某种方式互相连接起来，形成大脑内部的生物神经元网络，这些神经元又随着所接收到的多个激励信号的综合大小而呈现兴奋或抑制状态。</p>
<p>大脑的学习过程就是神经元之间连接强度随外部激励信息做自适应变化的过程，而大脑处理信息的结果则由神经元的状态表现出来。</p>
<ul>
<li><p>并行、分布处理结构</p>
</li>
<li><p>一个处理单元的输出可以被任意分枝，且大小不变</p>
</li>
<li><p>输出信号可以是任意的数学模型</p>
</li>
<li><p>处理单元完全的局部操作</p>
</li>
</ul>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906165352982.png" alt="image-20220906165352982" style="zoom: 67%;">

<h3 id="1-4人工神经网络的特点"><a href="#1-4人工神经网络的特点" class="headerlink" title="1.4人工神经网络的特点"></a>1.4人工神经网络的特点</h3><h4 id="固有的并行结构和并行处理特性"><a href="#固有的并行结构和并行处理特性" class="headerlink" title="固有的并行结构和并行处理特性"></a>固有的并行结构和并行处理特性</h4><p>ANN的计算功能分布在多个处理单元上，在同一层内的处理单元同时并行操作。ANN中的信息处理是在大量单元中并行而又分层进行的。</p>
<h4 id="知识的分布存储特点"><a href="#知识的分布存储特点" class="headerlink" title="知识的分布存储特点"></a>知识的分布存储特点</h4><p>在ANN中，知识不是存储在特定的存储单元中， 而是分布存储在整个网络的所有连接权中。</p>
<h4 id="良好的容错特性"><a href="#良好的容错特性" class="headerlink" title="良好的容错特性"></a>良好的容错特性</h4><p>当输入是一些模糊、变形等不完善的数据和信息时，ANN能够通过联想恢复完整的记忆，从而实现对不完整输入信息的正确识别。</p>
<h4 id="高度非线性及计算的非精确性"><a href="#高度非线性及计算的非精确性" class="headerlink" title="高度非线性及计算的非精确性"></a>高度非线性及计算的非精确性</h4><p>ANN的结构的并行性和知识的分布存储使其信息的存储与处理表现出了空间上分布，时间上并行的特点，使得网络非线性。</p>
<p>由于能够处理一些不精确、不完整的模糊信息， 所以解为满意解而非精确解。</p>
<h4 id="自学习、自组织和自适应性"><a href="#自学习、自组织和自适应性" class="headerlink" title="自学习、自组织和自适应性"></a>自学习、自组织和自适应性</h4><p>自学习是指当外部环境发生变化时，经过一段时间的训练或感知，神经网络能够对给定的输 入产生期望的输出。</p>
<p>自组织是神经网络通过训练可以自行调节连接权值，即调节神经元之间的突触连接，使其具有可塑性，以逐步构建适应于不同信息处理要求的神经网络。</p>
<h4 id="联想记忆"><a href="#联想记忆" class="headerlink" title="联想记忆"></a>联想记忆</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906175756251.png" alt="image-20220906175756251" style="zoom: 67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906175830366.png" alt="image-20220906175830366" style="zoom: 67%;">

<h3 id="1-6-ANN的应用领域"><a href="#1-6-ANN的应用领域" class="headerlink" title="1.6 ANN的应用领域"></a>1.6 ANN的应用领域</h3><ul>
<li><p>从样例中学习</p>
</li>
<li><p>文字处理</p>
</li>
<li><p>生物特征识别</p>
</li>
<li><p>生物医学</p>
</li>
<li><p>遥感</p>
</li>
<li><p>文档分类</p>
</li>
<li><p>预测决策</p>
</li>
<li><p>机器人</p>
</li>
</ul>
<h3 id="1-7-ANN面临的主要挑战"><a href="#1-7-ANN面临的主要挑战" class="headerlink" title="1.7 ANN面临的主要挑战"></a>1.7 ANN面临的主要挑战</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906165746110.png" alt="image-20220906165746110" style="zoom: 50%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906170050871.png" alt="image-20220906170050871" style="zoom: 67%;">

<p>不确切监督：任务力度需要很细，但是输入标签比较粗。比如任务需要语义分割，但是给的标签为点标注或线标注</p>
<h3 id="1-8ANN的前沿方向"><a href="#1-8ANN的前沿方向" class="headerlink" title="1.8ANN的前沿方向"></a>1.8ANN的前沿方向</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906180530291.png" alt="image-20220906180530291" style="zoom:67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906180645317.png" alt="image-20220906180645317" style="zoom:67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906180702858.png" alt="image-20220906180702858" style="zoom:67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906180727588.png" alt="image-20220906180727588" style="zoom:67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906165436756.png" alt="image-20220906165436756" style="zoom: 67%;">

<ul>
<li><p>图像分类</p>
</li>
<li><p>点标注</p>
</li>
<li><p>目标识别</p>
</li>
<li><p>线标注</p>
</li>
<li><p>语义分割</p>
</li>
</ul>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220906180823731.png" alt="image-20220906180823731" style="zoom:67%;">

<h2 id="Chapter-2-Artificial-Neural-Network"><a href="#Chapter-2-Artificial-Neural-Network" class="headerlink" title="Chapter 2 Artificial Neural Network"></a>Chapter 2 Artificial Neural Network</h2><h3 id="认知计算：基本神经信息处理机制"><a href="#认知计算：基本神经信息处理机制" class="headerlink" title="认知计算：基本神经信息处理机制"></a>认知计算：基本神经信息处理机制</h3><ul>
<li><strong>Vision视觉</strong>：CNN图片处理</li>
<li><strong>Attention注意力</strong>：attention mechanism权重</li>
<li>Dopamine and Reward多巴胺：奖惩，强化学习</li>
<li><strong>Memory记忆</strong>：RNN时间序列</li>
<li>Meaning意义</li>
<li>Task directed behavior任务导向型：adversarial生成对抗学习，feature特征解耦学习</li>
</ul>
<h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><ul>
<li>树突：接收</li>
<li>轴突：发射</li>
<li>突触：设定阈值（兴奋、抑制）</li>
<li>胞体：对输入信号求和</li>
</ul>
<h3 id="人工神经元"><a href="#人工神经元" class="headerlink" title="人工神经元"></a>人工神经元</h3><ul>
<li>Node：节点</li>
<li>Input：输入</li>
<li>Output：输出</li>
<li>bias：偏置</li>
<li>activation：激活函数</li>
<li>weight：权重</li>
</ul>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="阶跃函数"><a href="#阶跃函数" class="headerlink" title="阶跃函数"></a>阶跃函数</h4><p>二分类</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212111251132.png" alt="image-20230212111251132" style="zoom: 67%;">

<h4 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h4><p>分类具有线性特点，无法处理非线性问题</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212111621494.png" alt="image-20230212111621494" style="zoom:67%;">

<h4 id="sigmod"><a href="#sigmod" class="headerlink" title="sigmod"></a>sigmod</h4><p>用于转化线性至非线性，导数便于计算</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212111642485.png" alt="image-20230212111642485" style="zoom:67%;">

<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>线性转非线性，将输出值固定在一个区间。梯度爆炸消失，计算量较大</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212111701406.png" alt="image-20230212111701406" style="zoom:67%;">

<h4 id="leaky-relu"><a href="#leaky-relu" class="headerlink" title="leaky relu"></a>leaky relu</h4><p>relu函数 f(x)=max(0, x)</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212111722688.png" alt="image-20230212111722688" style="zoom:67%;">

<h2 id="Chapter-3-Learning-amp-Perceptron"><a href="#Chapter-3-Learning-amp-Perceptron" class="headerlink" title="Chapter 3 Learning &amp; Perceptron"></a>Chapter 3 Learning &amp; Perceptron</h2><p>输入层——隐藏层——输出层</p>
<p>信号逐层传递，同层之间无联系</p>
<p>使用$w_{x,y}$表示权重，其中x为后层编号，y为前层编号</p>
<h3 id="损失函数（单个样例的误差）"><a href="#损失函数（单个样例的误差）" class="headerlink" title="损失函数（单个样例的误差）"></a>损失函数（单个样例的误差）</h3><h4 id="0-1损失"><a href="#0-1损失" class="headerlink" title="0-1损失"></a>0-1损失</h4><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120707585.png" alt="image-20230212120707585"></p>
<h4 id="平方损失"><a href="#平方损失" class="headerlink" title="平方损失"></a>平方损失</h4><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120721320.png" alt="image-20230212120721320"></p>
<h4 id="绝对值损失"><a href="#绝对值损失" class="headerlink" title="绝对值损失"></a>绝对值损失</h4><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120727591.png" alt="image-20230212120727591"></p>
<h3 id="代价函数（训练样本集的平均误差）"><a href="#代价函数（训练样本集的平均误差）" class="headerlink" title="代价函数（训练样本集的平均误差）"></a>代价函数（训练样本集的平均误差）</h3><h4 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h4><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120744078.png" alt="image-20230212120744078"></p>
<h4 id="平均绝对误差"><a href="#平均绝对误差" class="headerlink" title="平均绝对误差"></a>平均绝对误差</h4><p>通常用于回归</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120751102.png" alt="image-20230212120751102"></p>
<h4 id="交叉熵代价函数（看录播）"><a href="#交叉熵代价函数（看录播）" class="headerlink" title="交叉熵代价函数（看录播）"></a>交叉熵代价函数（看录播）</h4><p>通常用于分类</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212120800700.png" alt="image-20230212120800700"></p>
<p>真实概率分布和预测概率分布尽可能相似接近</p>
<h3 id="Rosenblatt-Perceptron算法"><a href="#Rosenblatt-Perceptron算法" class="headerlink" title="Rosenblatt Perceptron算法"></a>Rosenblatt Perceptron算法</h3><p>采用阶跃激活（-1，1）</p>
<ol>
<li>Input：${(X_i,Y_i)}_{i=1\sim n}$</li>
<li>Random：(w, b)</li>
<li>Pick up ：$X_i$<ol>
<li>如果wx+b≥0且y=-1，即y(wx+b)&lt;0，则$W=W+\eta yX$，$b=b+\eta y$</li>
</ol>
</li>
<li>repeat stet(3)，直到所有的归类均正确</li>
</ol>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122051749.png" alt="image-20230212122051749">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122101217.png" alt="image-20230212122101217" style="zoom: 80%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122110137.png" alt="image-20230212122110137" style="zoom: 80%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122118876.png" alt="image-20230212122118876" style="zoom: 67%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122126180.png" alt="image-20230212122126180">

<h3 id="学习（看录播）"><a href="#学习（看录播）" class="headerlink" title="学习（看录播）"></a>学习（看录播）</h3><h4 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h4><p>方法：回归和分类</p>
<p>给出样本的标准答案，以答案为导向</p>
<h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><p>方法：聚类</p>
<p>以相似性为导向</p>
<p>误差为样本集到中心点的距离：欧氏距离，余弦相似度。</p>
<h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><p>R and P奖惩机制</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMG_20221011_152249_edit_7168777453592.jpg" alt="IMG_20221011_152249_edit_7168777453592" style="zoom:80%;">

<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122910585.png" alt="image-20230212122910585"></p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212122945064.png" alt="image-20230212122945064"></p>
<h2 id="Chapter-4-Optimization优化"><a href="#Chapter-4-Optimization优化" class="headerlink" title="Chapter 4 Optimization优化"></a>Chapter 4 Optimization优化</h2><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212145049179.png" alt="image-20230212145049179"></p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>选择初始点$x_0$，迭代直到收敛</p>
<p>$x_{t+1}=x_t-\eta \nabla f(x_t)$，其中$\eta$为学习率</p>
<p>可能的停止条件：迭代到$||\nabla f(x_t)||≤ \epsilon$，其中$\epsilon&gt;0$</p>
<p>学习率过大可能会跳过极小点，学习率过小会导致收敛太慢，前大后小。</p>
<h3 id="SGD随机梯度下降"><a href="#SGD随机梯度下降" class="headerlink" title="SGD随机梯度下降"></a>SGD随机梯度下降</h3><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212155049887.png" alt="image-20230212155049887"></p>
<h4 id="随机梯度下降法（Stochastic-Gradient-Descent-SGD）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent-SGD）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent, SGD）"></a>随机梯度下降法（Stochastic Gradient Descent, SGD）</h4><p>随机梯度下降算法每次从训练集中随机选择一个样本来进行学习（batch_size=1）。</p>
<p>优点：每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。</p>
<p>SGD波动带来的好处，在类似盆地区域，即很多局部极小值点，那么这个波动的特点可能会使得优化的方向从当前的局部极小值点调到另一个更好的局限极小值点，这样便可能对于非凹函数，最终收敛于一个较好的局部极值点，甚至全局极值点。</p>
<p>缺点：每次更新可能并不会按照正确的方向进行，因此会带来优化波动，使得迭代次数增多，即收敛速度变慢。</p>
<h4 id="批量梯度下降法（Batch-Gradient-Gradient-Descent-BGD）"><a href="#批量梯度下降法（Batch-Gradient-Gradient-Descent-BGD）" class="headerlink" title="批量梯度下降法（Batch Gradient Gradient Descent, BGD）"></a>批量梯度下降法（Batch Gradient Gradient Descent, BGD）</h4><p>每次使用全部的训练样本来更新模型参数/学习</p>
<p>优点：每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点</p>
<p>缺点：每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，不能进行在线模型参数更新。</p>
<h4 id="小批量梯度下降法（Mini-batch-Gradient-Descent-SGD）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent-SGD）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent, SGD）"></a>小批量梯度下降法（Mini-batch Gradient Descent, SGD）</h4><p>小批量梯度下降综合了batch梯度下降与stochastic梯度下降，在每次更新速度与更新次数中间实现一个平衡，其每次更新从训练集中随机选择k（k&lt;m）个样本进行学习。</p>
<p>优点：</p>
<ul>
<li>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定</li>
<li>相对于批量梯度下降，其提高了每次学习的速度；</li>
<li>MBGD不用担心内存瓶颈从而可以利用矩阵运算进行高效计算</li>
</ul>
<h3 id="优化类别"><a href="#优化类别" class="headerlink" title="优化类别"></a>优化类别</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMG_20221011_155717_edit_12861323015745.jpg" alt="IMG_20221011_155717_edit_12861323015745" style="zoom: 25%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMG_20221011_170834_edit_12795529270963.jpg" alt="IMG_20221011_170834_edit_12795529270963" style="zoom: 25%;">

<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>首先正向计算误差值，再反向调整权重值</p>
<p>计算误差，对权重求偏导，找到误差减小最快方向。</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221018145155481.png" alt="image-20221018145155481" style="zoom: 50%;">

<p>结点i的输入为$z_i$，结点i的输出为$y_i$</p>
<p>损失函数为$E=\frac{1}{2}(y_p-y_a)^2$，激活函数为$f(x)=\frac{1}{1+e^{-x}}$</p>
<p>则计算y5误差的计算过程为</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212194641045.png" alt="image-20230212194641045" style="zoom: 80%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212194700075.png" alt="image-20230212194700075" style="zoom:80%;">

<p>利用链式法则求$W_{53}$的偏导，从而对$w_{53}$进行权值修正，根据前向计算的公式，有</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212200543200.png" alt="image-20230212200543200"><br>$$<br>\Delta w_{53}=\frac{\delta E}{\delta w_{53}}=\frac{\delta E}{\delta y_5}\frac{\delta y_5}{\delta z_5}\frac{\delta z_5}{\delta w_{53}}=(y_5-y_a)<em>(f(z_5)</em>(1-f(z_5)))<em>y_3=(0.69-0.5)</em>(0.69*(1-0.69))*0.663=0.2711<br>$$<br><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221018153549593.png" alt="image-20221018153549593" style="zoom:67%;"></p>
<p>在后面计算例如$w_{31}$得到权值修正时，会遇到$w_{53}$参与计算，这时需要一次性更新，即使用旧的$w_{53}$值，因为$W_{31}$计算过程中的其他参数是基于更新前的数值进行的。</p>
<h2 id="Chapter-5-Convolutional-Neural-Network"><a href="#Chapter-5-Convolutional-Neural-Network" class="headerlink" title="Chapter 5 Convolutional Neural Network"></a>Chapter 5 Convolutional Neural Network</h2><h3 id="ConvNet-Topology卷积网络拓扑"><a href="#ConvNet-Topology卷积网络拓扑" class="headerlink" title="ConvNet Topology卷积网络拓扑"></a>ConvNet Topology卷积网络拓扑</h3><p>多层感知器的改进</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212230119259.png" alt="image-20230212230119259" style="zoom:67%;">

<p>CNN是一个前馈神经网络，可以从一个图像中提取拓扑属性，采用反向传播算法训练减少误差</p>
<p>卷积神经网络旨在通过最少的预处理直接从像素图像中识别视觉模式，可以识别多变性图像</p>
<p>由卷积层、下采样层（池化层）和全连接层组成</p>
<p>C卷积层，局部感知，提取局部特征。权重共享。</p>
<p>P池化层，S下采样</p>
<p>F全连接层</p>
<h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p>每个神经元仅与输入神经元的一块区域连接，称为感受野。在图像卷积操作中，神经元在空间维度是局部连接，但在深度上是全部连接。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有较强的响应。</p>
<p>下图为filters卷积核可视化图像</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230212232527385.png" alt="卷积核可视化"></p>
<p>卷积核与图像某一与卷积核相同大小的区域内各个像素点依次相乘后相加求和</p>
<p>stride步长：卷积核每次平移的距离<br>$$<br>featuremap_size=\lfloor\frac{image_size-kernel_size+2*padsize}{stride}\rfloor+1<br>$$</p>
<h3 id="权重共享"><a href="#权重共享" class="headerlink" title="权重共享"></a>权重共享</h3><p>权重对于同一深度切片的神经元是共享的，在很大程度上减少参数。共享权重使得图片的底层边缘特征与特征在图中的具体位置无关。在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征。</p>
<p>不同于全连接，卷积核只与部分像素点进行权重计算，并且同一卷积核采用相同权重。</p>
<h3 id="池化Pooling"><a href="#池化Pooling" class="headerlink" title="池化Pooling"></a>池化Pooling</h3><p>减少原有图像对应像素点的值，保留图像的相对空间位置，减少计算量。</p>
<ul>
<li>Max pooling最大池化</li>
<li>Average pooling平均池化</li>
<li>Norm pooling范数池化<ul>
<li>一范数：绝对值之和</li>
<li>二范数：平方和开根号</li>
<li>p-范数：向量元素绝对值的p次方和的1/p次幂</li>
<li>无穷范数：所有向量元素绝对值中的最大值</li>
</ul>
</li>
<li>Log probability pooling对数概率池化</li>
</ul>
<p>多层卷积的意义：将不同方向上的特征高阶融合</p>
<h3 id="平整化Flattening"><a href="#平整化Flattening" class="headerlink" title="平整化Flattening"></a>平整化Flattening</h3><p>将矩阵拉成一个列向量，后放入全连接层。</p>
<h3 id="CNN计算方面"><a href="#CNN计算方面" class="headerlink" title="CNN计算方面"></a>CNN计算方面</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213113959219.png" alt="image-20230213113959219" style="zoom:80%;">

<p>Convolution2D(25, 3, 3)表示有25个大小为3*3的过滤</p>
<p>input_shape(28, 28, 1)表示输入为28*28大小，1为黑白，3为RGB</p>
<p>MaxPooling2D(2,2)表示最大池化层为2*2大小</p>
<p>卷积核的个数决定通道数</p>
<p>Pooling向下取整</p>
<p>25*13*13进行convolution操作，卷积核为50*3*3，首先对于50个卷积核，每个卷积核与25层的13*13大小的像素卷积，后将25层信息叠加为一层，以此得到50*11*11的卷积结果</p>
<h3 id="LeNet网络"><a href="#LeNet网络" class="headerlink" title="LeNet网络"></a>LeNet网络</h3><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213115047213.png" alt="image-20230213115047213"></p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213114922447.png" alt="image-20230213114922447"></p>
<p>最后RBF输出为10，是因为该问题为数字识别，仅有0~9共10个数字，即为分类问题，输出概率最高数字。</p>
<h3 id="AlexNet网络"><a href="#AlexNet网络" class="headerlink" title="AlexNet网络"></a>AlexNet网络</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213115632021.png" alt="image-20230213115632021" style="zoom: 67%;">

<p>NORM层：归一化、正则化层</p>
<h4 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101144312273.png" alt="image-20221101144312273" style="zoom:50%;">

<ul>
<li>使用relu非线性激活函数代替sigmoid激活函数</li>
<li>使用dropout操作<ul>
<li>每一次迭代训练中随机让一些神经元处于不激活状态，避免神经网络过拟合情况</li>
</ul>
</li>
<li>使用最大池化而不是平均池化</li>
<li>采用LRN<ul>
<li>local response normalization局部响应归一化</li>
</ul>
</li>
<li>采用梯度下降动量法</li>
</ul>
<h4 id="相关参数计算"><a href="#相关参数计算" class="headerlink" title="相关参数计算"></a>相关参数计算</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101142436368.png" alt="image-20221101142436368" style="zoom: 50%;">

<p>第一层输出大小为(227-11)/4+1=55</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101142626659.png" alt="image-20221101142626659" style="zoom: 67%;">

<p>96 * (3 * 11 * 11) = 34848个权重</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101143503662.png" alt="image-20221101143503662" style="zoom: 33%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101143515146.png" alt="image-20221101143515146" style="zoom: 50%;">

<p>池化层没有使用权重，为0！</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101143743262.png" alt="image-20221101143743262" style="zoom:50%;">

<h3 id="VGGNet网络"><a href="#VGGNet网络" class="headerlink" title="VGGNet网络"></a>VGGNet网络</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101150403091.png" alt="image-20221101150403091" style="zoom: 50%;">

<p>卷积核越来越小，深度越来越深</p>
<p>层过多时：过拟合梯度消失、退化问题严重</p>
<h3 id="GoogLeNet网络"><a href="#GoogLeNet网络" class="headerlink" title="GoogLeNet网络"></a>GoogLeNet网络</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101151522681.png" alt="image-20221101151522681" style="zoom: 50%;">

<p>使用不同大小的卷积核(1 * 1，3 * 3，5 * 5)进行级联，提取不同大小细节特征</p>
<p>之后进行Pooling(3 * 3)</p>
<p>concatenation实现通道拼接，将输入<strong>填充</strong>成相同大小图像，通道数叠加。</p>
<p>但计算量大幅度提高</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101153002004.png" alt="image-20221101153002004" style="zoom: 67%;">

<h3 id="ResNet网络"><a href="#ResNet网络" class="headerlink" title="ResNet网络"></a>ResNet网络</h3><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101155057534.png" alt="image-20221101155057534"></p>
<h4 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h4><p>输入和输入的非线性变化的叠加<br>$$<br>x_{l+1}=x_l+F(x_l,W_l)<br>$$<br><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213131714718.png" alt="image-20230213131714718" style="zoom: 50%;"></p>
<p>残差块分为两部分，直接映射部分和残差部分。weight代表卷积，addition代表单位加操作。</p>
<p>如果对函数求导，可以看出x的导数为1，另外的F（x，w）导数不可能一直为-1，所以不会出现梯度消失问题。</p>
<p>在卷积网络中$x_l$和$x_{l+1}$可能存在维度不一样的问题，对x进行1*1卷积便于运算，即$x_{l+1}=h(x_l)+F(x_l,W_l)$，其中h为1*1卷积运算。</p>
<p>优化器选择是否进行残差连接操作</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101161712949.png" alt="image-20221101161712949"></p>
<h3 id="DenseNet网络"><a href="#DenseNet网络" class="headerlink" title="DenseNet网络"></a>DenseNet网络</h3><p>与ResNet前一层与后一层的短路连接不同，DenseNet建立密集连接，每个层都会与前面所有层在channel维度上连接在一起。</p>
<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221101163008045.png" alt="image-20221101163008045"></p>
<p>CNN网络一般采用Pooling或者Conv层来降低特征图的大小，而DenseNet采用DenseBlock+Transition的结构。每个层的特征图大小相同，层与层之间采用密集连接方式。</p>
<h4 id="DenseBlock内部结构"><a href="#DenseBlock内部结构" class="headerlink" title="DenseBlock内部结构"></a>DenseBlock内部结构</h4><p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213134435468.png" alt="image-20230213134435468"></p>
<h2 id="Chapter-6-Recurrent-Neural-Network"><a href="#Chapter-6-Recurrent-Neural-Network" class="headerlink" title="Chapter 6 Recurrent Neural Network"></a>Chapter 6 Recurrent Neural Network</h2><h3 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h3><p>时间序列数据是指在不同时间点采集到的数据，它反映了某一事物或现象随时间变化的状态或程度。</p>
<p>顺序数据可能不会随时间变化(例如文本序列)，但顺序数据总是有一个特征:后一个数据与前一个数据相关。</p>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213140422736.png" alt="image-20230213140422736" style="zoom: 67%;">

<ul>
<li>X：input layer neuron value (它表示输入层的值）</li>
<li>U：the weight between input layer to hidden layer(输入层到隐藏层的权重)</li>
<li>S：the hidden layer neuron value(它表示隐藏层)</li>
<li>O：the output layer neuron value(它表示输出层)</li>
<li>V：the weight between hidden layer to input layer(隐藏层到输出层的权重)</li>
</ul>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213140838344.png" alt="image-20230213140838344" style="zoom: 80%;">
$$
O_t=g(V*S_t)
$$

<p>$$<br>S_t=f(U<em>X_i+W</em>S_{t-1})<br>$$</p>
<p>$S_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$</p>
<h4 id="序列过程"><a href="#序列过程" class="headerlink" title="序列过程"></a>序列过程</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213141419614.png" alt="image-20230213141419614" style="zoom: 80%;">

<h4 id="小练习"><a href="#小练习" class="headerlink" title="小练习"></a>小练习</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213143213033.png" alt="image-20230213143213033" style="zoom: 50%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213142807458.png" alt="image-20230213142807458" style="zoom: 67%;">

<p>经过三个时刻的变化，求出第一时刻、第二时刻、第三时刻的y1、y2的输出</p>
<p> 答案为：[4,4]-&gt;[12,12]-&gt;[32,32]</p>
<h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory(LSTM)"></a>Long Short Term Memory(LSTM)</h3><h4 id="记忆单元memory-cell"><a href="#记忆单元memory-cell" class="headerlink" title="记忆单元memory cell"></a>记忆单元memory cell</h4><center class="half">
    <img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115141950705.png" alt="image-20221115141950705" style="zoom:60%;">
    <img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115142111172.png" alt="image-20221115142111172" style="zoom: 40%;">
</center>

<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115142304787.png" alt="image-20221115142304787" style="zoom: 50%;">

<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20230213144711701.png" alt="image-20230213144711701">

<h4 id="gate门单元"><a href="#gate门单元" class="headerlink" title="gate门单元"></a>gate门单元</h4><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115142527715.png" alt="image-20221115142527715" style="zoom: 50%;">

<h5 id="forget-gate"><a href="#forget-gate" class="headerlink" title="forget gate"></a>forget gate</h5><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115143019935.png" alt="image-20221115143019935" style="zoom: 50%;">

<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115143414868.png" alt="image-20221115143414868"></p>
<p>历史信息与当下信息的相关性</p>
<h5 id="input-gate"><a href="#input-gate" class="headerlink" title="input gate"></a>input gate</h5><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115143651903.png" alt="image-20221115143651903" style="zoom:50%;">

<p>输入信息与历史信息的相关性</p>
<img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115144733016.png" alt="image-20221115144733016" style="zoom:50%;">

<p><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115145010710.png" alt="image-20221115145010710"></p>
<h5 id="output-gate"><a href="#output-gate" class="headerlink" title="output gate"></a>output gate</h5><img src="/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221115145329085.png" alt="image-20221115145329085" style="zoom:50%;">



<p><strong>bos</strong> begin of sentense</p>
<p><strong>ens</strong> end of sentense</p>
<h1 id="再回顾"><a href="#再回顾" class="headerlink" title="再回顾"></a>再回顾</h1><p>上面的内容因为是课堂随笔记的，显得有些乱，不如直接看课件。下面的内容为复习时的所思所想</p>
<h4 id="激活函数优缺点"><a href="#激活函数优缺点" class="headerlink" title="激活函数优缺点"></a>激活函数优缺点</h4><p>优点：可以将连续值映射到0到1之间，将问题转化为概率问题，值大于0.5时是正例。<br>缺点：可能导致梯度消失和梯度爆炸问题；因为其中含有幂运算，所以计算机处理时会很耗时。</p>
<h4 id="什么是线性可分离和不可分离"><a href="#什么是线性可分离和不可分离" class="headerlink" title="什么是线性可分离和不可分离"></a>什么是线性可分离和不可分离</h4><p>假设现在有一堆训练数据，它们是由两类点组成的，其中一类点用三角符号画在坐标系中，另一类点用圆圈画在坐标系中。现在拿出直尺和笔在坐标系中画一条直线，如果可以把三角点分到直线的一边，圆圈点分到直线的另一边的话，就说这个训练数据是线性可分的；否则，数据就是线性不可分的。</p>
<h4 id="深度学习中epoch、iteration和batchsize的含义"><a href="#深度学习中epoch、iteration和batchsize的含义" class="headerlink" title="深度学习中epoch、iteration和batchsize的含义"></a>深度学习中epoch、iteration和batchsize的含义</h4><h5 id="batchsize批大小"><a href="#batchsize批大小" class="headerlink" title="batchsize批大小"></a>batchsize批大小</h5><p>batch_size将影响到模型的优化程度和速度，为了在内存效率和内存容量之间寻找最佳平衡。</p>
<p>适当的增加Batch_Size的优点：</p>
<ul>
<li>通过并行化提高内存利用率。</li>
<li>单次epoch的迭代次数减少，提高运行速度。（单次epoch=(全部训练样本/batchsize)/iteration=1）</li>
<li>适当的增加Batch_Size,梯度下降方向准确度增加，训练震动的幅度减小。</li>
</ul>
<h5 id="iteration迭代"><a href="#iteration迭代" class="headerlink" title="iteration迭代"></a>iteration迭代</h5><p>1个iteration等于使用batchsize个样本训练一次，训练过程为一个正向传递和一个反向传递。</p>
<h5 id="epoch时期"><a href="#epoch时期" class="headerlink" title="epoch时期"></a>epoch时期</h5><p>1个epoch等于使用训练集中的全部样本训练一次，即所有训练样本的一个正向传递和一个反向传递</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">萌新QAQ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://jszmwq.github.io/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://jszmwq.github.io/2022/09/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://jszmwq.github.io" target="_blank">j52nnw9的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%A7%91%E8%AF%BE%E7%A8%8B/">计科课程</a></div><div class="post_share"><div class="social-share" data-image="/img/Animation/5.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/06/%E6%95%B0%E7%90%86%E9%80%BB%E8%BE%91-%E5%8F%AF%E4%BF%A1AI%E7%AC%94%E8%AE%B0/"><img class="prev-cover" src="/img/Animation/1.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数理逻辑-可信AI笔记</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/14/ardupilot%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><img class="next-cover" src="/img/Animation/7.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ardupilot学习笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/31/ChatGPT%E5%AF%B9%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E7%9A%84%E5%BD%B1%E5%93%8D/" title="ChatGPT对网络空间安全的影响"><img class="cover" src="/img/Animation/1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-31</div><div class="title">ChatGPT对网络空间安全的影响</div></div></a></div><div><a href="/2022/12/21/pintos%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C/" title="pintos操作系统实验"><img class="cover" src="/img/Animation/4.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-21</div><div class="title">pintos操作系统实验</div></div></a></div><div><a href="/2022/12/02/%E5%90%8C%E6%9E%84%E7%BD%91%E7%BB%9C%E5%8F%AF%E5%88%86%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98/" title="同构网络可分任务调度问题"><img class="cover" src="/img/Animation/1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-02</div><div class="title">同构网络可分任务调度问题</div></div></a></div><div><a href="/2022/11/22/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E5%BB%BA%E6%A8%A1%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/" title="大数据优化建模与算法笔记"><img class="cover" src="/img/Animation/5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-22</div><div class="title">大数据优化建模与算法笔记</div></div></a></div><div><a href="/2023/02/23/%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/" title="形式语言与自动机"><img class="cover" src="/img/Animation/9.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-23</div><div class="title">形式语言与自动机</div></div></a></div><div><a href="/2022/12/14/%E6%80%8E%E6%A0%B7%E4%BD%BF%E7%94%A8ARENA%E7%B3%BB%E7%BB%9F/" title="怎样使用ARENA系统"><img class="cover" src="/img/Animation/2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">怎样使用ARENA系统</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">萌新QAQ</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">神经网络 第一章 人工神经网络绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">1.1人工智能的含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.</span> <span class="toc-text">1.3人工神经网络的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text">1.4人工神经网络的特点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BA%E6%9C%89%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%BB%93%E6%9E%84%E5%92%8C%E5%B9%B6%E8%A1%8C%E5%A4%84%E7%90%86%E7%89%B9%E6%80%A7"><span class="toc-number">1.3.1.</span> <span class="toc-text">固有的并行结构和并行处理特性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E7%9A%84%E5%88%86%E5%B8%83%E5%AD%98%E5%82%A8%E7%89%B9%E7%82%B9"><span class="toc-number">1.3.2.</span> <span class="toc-text">知识的分布存储特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%89%AF%E5%A5%BD%E7%9A%84%E5%AE%B9%E9%94%99%E7%89%B9%E6%80%A7"><span class="toc-number">1.3.3.</span> <span class="toc-text">良好的容错特性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E5%BA%A6%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%8A%E8%AE%A1%E7%AE%97%E7%9A%84%E9%9D%9E%E7%B2%BE%E7%A1%AE%E6%80%A7"><span class="toc-number">1.3.4.</span> <span class="toc-text">高度非线性及计算的非精确性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AD%A6%E4%B9%A0%E3%80%81%E8%87%AA%E7%BB%84%E7%BB%87%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E6%80%A7"><span class="toc-number">1.3.5.</span> <span class="toc-text">自学习、自组织和自适应性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%94%E6%83%B3%E8%AE%B0%E5%BF%86"><span class="toc-number">1.3.6.</span> <span class="toc-text">联想记忆</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-ANN%E7%9A%84%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">1.4.</span> <span class="toc-text">1.6 ANN的应用领域</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-ANN%E9%9D%A2%E4%B8%B4%E7%9A%84%E4%B8%BB%E8%A6%81%E6%8C%91%E6%88%98"><span class="toc-number">1.5.</span> <span class="toc-text">1.7 ANN面临的主要挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8ANN%E7%9A%84%E5%89%8D%E6%B2%BF%E6%96%B9%E5%90%91"><span class="toc-number">1.6.</span> <span class="toc-text">1.8ANN的前沿方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-2-Artificial-Neural-Network"><span class="toc-number">2.</span> <span class="toc-text">Chapter 2 Artificial Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A4%E7%9F%A5%E8%AE%A1%E7%AE%97%EF%BC%9A%E5%9F%BA%E6%9C%AC%E7%A5%9E%E7%BB%8F%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.</span> <span class="toc-text">认知计算：基本神经信息处理机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">2.2.</span> <span class="toc-text">神经元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">2.3.</span> <span class="toc-text">人工神经元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.1.</span> <span class="toc-text">阶跃函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.2.</span> <span class="toc-text">线性函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sigmod"><span class="toc-number">2.4.3.</span> <span class="toc-text">sigmod</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tanh"><span class="toc-number">2.4.4.</span> <span class="toc-text">tanh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#leaky-relu"><span class="toc-number">2.4.5.</span> <span class="toc-text">leaky relu</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-3-Learning-amp-Perceptron"><span class="toc-number">3.</span> <span class="toc-text">Chapter 3 Learning &amp; Perceptron</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88%E5%8D%95%E4%B8%AA%E6%A0%B7%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">损失函数（单个样例的误差）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0-1%E6%8D%9F%E5%A4%B1"><span class="toc-number">3.1.1.</span> <span class="toc-text">0-1损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="toc-number">3.1.2.</span> <span class="toc-text">平方损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E5%80%BC%E6%8D%9F%E5%A4%B1"><span class="toc-number">3.1.3.</span> <span class="toc-text">绝对值损失</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E9%9B%86%E7%9A%84%E5%B9%B3%E5%9D%87%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">代价函数（训练样本集的平均误差）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.2.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.2.2.</span> <span class="toc-text">平均绝对误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88%E7%9C%8B%E5%BD%95%E6%92%AD%EF%BC%89"><span class="toc-number">3.2.3.</span> <span class="toc-text">交叉熵代价函数（看录播）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rosenblatt-Perceptron%E7%AE%97%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">Rosenblatt Perceptron算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%9C%8B%E5%BD%95%E6%92%AD%EF%BC%89"><span class="toc-number">3.4.</span> <span class="toc-text">学习（看录播）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.1.</span> <span class="toc-text">有监督学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.2.</span> <span class="toc-text">无监督学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.3.</span> <span class="toc-text">强化学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-4-Optimization%E4%BC%98%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">Chapter 4 Optimization优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.1.</span> <span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.3.</span> <span class="toc-text">SGD随机梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Stochastic-Gradient-Descent-SGD%EF%BC%89"><span class="toc-number">4.3.1.</span> <span class="toc-text">随机梯度下降法（Stochastic Gradient Descent, SGD）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Batch-Gradient-Gradient-Descent-BGD%EF%BC%89"><span class="toc-number">4.3.2.</span> <span class="toc-text">批量梯度下降法（Batch Gradient Gradient Descent, BGD）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Mini-batch-Gradient-Descent-SGD%EF%BC%89"><span class="toc-number">4.3.3.</span> <span class="toc-text">小批量梯度下降法（Mini-batch Gradient Descent, SGD）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%B1%BB%E5%88%AB"><span class="toc-number">4.4.</span> <span class="toc-text">优化类别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">4.5.</span> <span class="toc-text">反向传播算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-5-Convolutional-Neural-Network"><span class="toc-number">5.</span> <span class="toc-text">Chapter 5 Convolutional Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNet-Topology%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91"><span class="toc-number">5.1.</span> <span class="toc-text">ConvNet Topology卷积网络拓扑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5"><span class="toc-number">5.2.</span> <span class="toc-text">局部连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-number">5.3.</span> <span class="toc-text">权重共享</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96Pooling"><span class="toc-number">5.4.</span> <span class="toc-text">池化Pooling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E6%95%B4%E5%8C%96Flattening"><span class="toc-number">5.5.</span> <span class="toc-text">平整化Flattening</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E8%AE%A1%E7%AE%97%E6%96%B9%E9%9D%A2"><span class="toc-number">5.6.</span> <span class="toc-text">CNN计算方面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.7.</span> <span class="toc-text">LeNet网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AlexNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.8.</span> <span class="toc-text">AlexNet网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">5.8.1.</span> <span class="toc-text">创新点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">5.8.2.</span> <span class="toc-text">相关参数计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VGGNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.9.</span> <span class="toc-text">VGGNet网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GoogLeNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.10.</span> <span class="toc-text">GoogLeNet网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.11.</span> <span class="toc-text">ResNet网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="toc-number">5.11.1.</span> <span class="toc-text">残差连接</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DenseNet%E7%BD%91%E7%BB%9C"><span class="toc-number">5.12.</span> <span class="toc-text">DenseNet网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DenseBlock%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84"><span class="toc-number">5.12.1.</span> <span class="toc-text">DenseBlock内部结构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-6-Recurrent-Neural-Network"><span class="toc-number">6.</span> <span class="toc-text">Chapter 6 Recurrent Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">6.1.</span> <span class="toc-text">序列数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.2.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E8%BF%87%E7%A8%8B"><span class="toc-number">6.2.1.</span> <span class="toc-text">序列过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%83%E4%B9%A0"><span class="toc-number">6.2.2.</span> <span class="toc-text">小练习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Long-Short-Term-Memory-LSTM"><span class="toc-number">6.3.</span> <span class="toc-text">Long Short Term Memory(LSTM)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E5%8D%95%E5%85%83memory-cell"><span class="toc-number">6.3.1.</span> <span class="toc-text">记忆单元memory cell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">6.3.2.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gate%E9%97%A8%E5%8D%95%E5%85%83"><span class="toc-number">6.3.3.</span> <span class="toc-text">gate门单元</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#forget-gate"><span class="toc-number">6.3.3.1.</span> <span class="toc-text">forget gate</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#input-gate"><span class="toc-number">6.3.3.2.</span> <span class="toc-text">input gate</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#output-gate"><span class="toc-number">6.3.3.3.</span> <span class="toc-text">output gate</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%8D%E5%9B%9E%E9%A1%BE"><span class="toc-number"></span> <span class="toc-text">再回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">0.0.1.</span> <span class="toc-text">激活函数优缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E7%A6%BB%E5%92%8C%E4%B8%8D%E5%8F%AF%E5%88%86%E7%A6%BB"><span class="toc-number">0.0.2.</span> <span class="toc-text">什么是线性可分离和不可分离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADepoch%E3%80%81iteration%E5%92%8Cbatchsize%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">0.0.3.</span> <span class="toc-text">深度学习中epoch、iteration和batchsize的含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#batchsize%E6%89%B9%E5%A4%A7%E5%B0%8F"><span class="toc-number">0.0.3.1.</span> <span class="toc-text">batchsize批大小</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#iteration%E8%BF%AD%E4%BB%A3"><span class="toc-number">0.0.3.2.</span> <span class="toc-text">iteration迭代</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#epoch%E6%97%B6%E6%9C%9F"><span class="toc-number">0.0.3.3.</span> <span class="toc-text">epoch时期</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/02/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%8A%E6%9C%BA%E5%AE%9E%E9%AA%8C%E5%9B%9B/" title="操作系统上机实验四"><img src="/img/Animation/4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="操作系统上机实验四"/></a><div class="content"><a class="title" href="/2022/06/02/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%8A%E6%9C%BA%E5%AE%9E%E9%AA%8C%E5%9B%9B/" title="操作系统上机实验四">操作系统上机实验四</a><time datetime="2023-04-23T09:53:46.388Z" title="更新于 2023-04-23 17:53:46">2023-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/31/%E8%87%AA%E9%80%82%E5%BA%94%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" title="自适应遗传算法"><img src="/img/Animation/3.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自适应遗传算法"/></a><div class="content"><a class="title" href="/2022/12/31/%E8%87%AA%E9%80%82%E5%BA%94%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" title="自适应遗传算法">自适应遗传算法</a><time datetime="2023-04-23T09:52:52.278Z" title="更新于 2023-04-23 17:52:52">2023-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E4%B8%8A%E6%9C%BA%E9%A2%98%E7%9B%AE/" title="计算机图形学上机题目"><img src="/img/Animation/4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机图形学上机题目"/></a><div class="content"><a class="title" href="/2023/01/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E4%B8%8A%E6%9C%BA%E9%A2%98%E7%9B%AE/" title="计算机图形学上机题目">计算机图形学上机题目</a><time datetime="2023-04-23T09:52:52.278Z" title="更新于 2023-04-23 17:52:52">2023-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/" title="计算机图形学"><img src="/img/Animation/5.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机图形学"/></a><div class="content"><a class="title" href="/2023/01/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/" title="计算机图形学">计算机图形学</a><time datetime="2023-04-23T09:52:52.278Z" title="更新于 2023-04-23 17:52:52">2023-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/22/%E8%AE%A1%E7%BB%84%E8%AF%BE%E8%AE%BE/" title="计组课设"><img src="/img/Animation/2.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计组课设"/></a><div class="content"><a class="title" href="/2022/12/22/%E8%AE%A1%E7%BB%84%E8%AF%BE%E8%AE%BE/" title="计组课设">计组课设</a><time datetime="2023-04-23T09:52:52.278Z" title="更新于 2023-04-23 17:52:52">2023-04-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 萌新QAQ</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"></script> <script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/hideMobileSidebar.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>